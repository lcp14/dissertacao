{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T23:30:10.271259Z",
     "start_time": "2019-03-28T23:30:10.246856Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/leandror/new_corpus.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-36d1cbbac61f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'datasets/artigosPre.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/leandror/new_corpus.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m17236\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m19610\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/leandror/new_corpus.txt'"
     ]
    }
   ],
   "source": [
    "open('datasets/artigosPre.txt','w').write((\"\\n\".join(open('/home/leandror/new_corpus.txt','r').read().split('\\n')[17236:19610])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T23:09:34.556095Z",
     "start_time": "2019-03-28T23:09:34.551551Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/home/leandror/cluwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T23:09:44.888124Z",
     "start_time": "2019-03-28T23:09:35.228655Z"
    }
   },
   "outputs": [],
   "source": [
    "import cluwords\n",
    "from cluwords import Cluwords, CluwordsTFIDF\n",
    "import embedding\n",
    "from embedding import CreateEmbeddingModels\n",
    "from metrics import Evaluation\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T23:10:20.393294Z",
     "start_time": "2019-03-28T23:10:20.376276Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_embedding_models(dataset, embedding_file_path, embedding_type,\n",
    "                            datasets_path, path_to_save_model):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Create the word2vec models for each dataset\n",
    "    \"\"\"\n",
    "    word2vec_models = CreateEmbeddingModels(embedding_file_path=embedding_file_path,\n",
    "                                            embedding_type=embedding_type,\n",
    "                                            document_path=datasets_path,\n",
    "                                            path_to_save_model=path_to_save_model)\n",
    "    n_words = word2vec_models.create_embedding_models(dataset)\n",
    "\n",
    "    return n_words\n",
    "def top_words(model, feature_names, n_top_words):\n",
    "    topico = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top = ''\n",
    "        top2 = ''\n",
    "        top += ' '.join([feature_names[i]\n",
    "                         for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        top2 += ''.join(str(sorted(topic)[:-n_top_words - 1:-1]))\n",
    "\n",
    "        topico.append(str(top))\n",
    "\n",
    "    return topico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T23:10:21.054153Z",
     "start_time": "2019-03-28T23:10:21.046121Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_results(model, tfidf_feature_names, cluwords_freq, cluwords_docs,\n",
    "                  dataset, path_to_save_results, path_to_save_model):\n",
    "    print(path_to_save_results)\n",
    "    for t in [5, 10, 20]:\n",
    "        with open('{}/result_topic_{}.txt'.format(path_to_save_results, t), 'w') as f_res:\n",
    "            f_res.write('Topics {}\\n'.format(t))\n",
    "            topics = top_words(model, tfidf_feature_names, t)\n",
    "            f_res.write('{}\\n'.format(topics))\n",
    "\n",
    "            coherence = Evaluation.coherence(topics, cluwords_freq, cluwords_docs)\n",
    "            f_res.write('Coherence: {} ({})\\n'.format(np.round(np.mean(coherence), 4), np.round(np.std(coherence), 4)))\n",
    "            f_res.write('{}\\n'.format(coherence))\n",
    "\n",
    "            pmi, npmi = Evaluation.pmi(topics, cluwords_freq, cluwords_docs,\n",
    "                                       sum([freq for word, freq in cluwords_freq.items()]), t)\n",
    "            f_res.write('PMI: {} ({})\\n'.format(np.round(np.mean(pmi), 4), np.round(np.std(pmi), 4)))\n",
    "            f_res.write('{}\\n'.format(pmi))\n",
    "            f_res.write('NPMI: {} ({})\\n'.format(np.round(np.mean(npmi), 4), np.round(np.std(npmi), 4)))\n",
    "            f_res.write('{}\\n'.format(npmi))\n",
    "\n",
    "            w2v_l1 = Evaluation.w2v_metric(topics, t, path_to_save_model, 'l1_dist', dataset)\n",
    "            f_res.write('W2V-L1: {} ({})\\n'.format(np.round(np.mean(w2v_l1), 4), np.round(np.std(w2v_l1), 4)))\n",
    "            f_res.write('{}\\n'.format(w2v_l1))\n",
    "\n",
    "            f_res.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T00:12:52.245924Z",
     "start_time": "2019-03-29T00:12:52.236903Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variaveis que voce deve alterar:\n",
    "DATASETS_PATH = \"\"\"datasets\"\"\"\n",
    "PATH_TO_SAVE_RESULTS = \"\"\"results\"\"\"\n",
    "#PATH_TO_SAVE_MODEL = \"\"\"word_emb_models/dataset_artigos_models\"\"\"\n",
    "EMBEDDINGS_FILE_PATH = \"\"\"word_emb_models/l2v.vec\"\"\"\n",
    "DATASET = \"artigos\"\n",
    "N_THREADS = 4\n",
    "N_COMPONENTS = 10\n",
    "\n",
    "# Nao precisa alterar essas:\n",
    "HAS_CLASS = False\n",
    "CLASS_PATH = \"\"\"\"\"\"\n",
    "EMBEDDINGS_BIN_TYPE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T23:32:04.564462Z",
     "start_time": "2019-03-28T23:32:03.056475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model read in 0.900s.\n",
      "artigos: 3965\n"
     ]
    }
   ],
   "source": [
    "# # RUN ONE TIME\n",
    "# n_words = create_embedding_models(dataset=DATASET,\n",
    "#                                   embedding_file_path=EMBEDDINGS_FILE_PATH,\n",
    "#                                   embedding_type=EMBEDDINGS_BIN_TYPE,\n",
    "#                                   datasets_path=DATASETS_PATH,\n",
    "#                                   path_to_save_model=PATH_TO_SAVE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T23:59:07.100728Z",
     "start_time": "2019-03-28T23:59:07.088796Z"
    }
   },
   "outputs": [],
   "source": [
    "n_words = 7686"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:17:57.707905Z",
     "start_time": "2019-03-29T01:17:57.690218Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "cossine_filter = 0.9\n",
    "word_count = n_words\n",
    "k = n_words\n",
    "algorithm_type = \"knn_cosine\"\n",
    "embedding_file_path = \"\"\"word_emb_models/l2v.vec\"\"\"\n",
    "dataset_file_path = \"\"\"{}/{}Pre.txt\"\"\".format(DATASETS_PATH, DATASET)\n",
    "path_to_save_results = '{}/{}'.format(PATH_TO_SAVE_RESULTS, DATASET)\n",
    "import os\n",
    "try:\n",
    "    os.mkdir('{}'.format(path_to_save_results))\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:22:14.659214Z",
     "start_time": "2019-03-29T01:18:03.492284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN...\n",
      "N Threads: 4\n",
      "NearestNeighbors K=7686\n",
      "Time 0.0009948150000127498\n",
      "NN Distaces\n",
      "Time 6.685157496000102\n",
      "Saving cluwords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cluwords.Cluwords at 0x7f53fe98bb00>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Codigo das Cluwords\n",
    "Cluwords(algorithm=algorithm_type,\n",
    "         embedding_file_path=embedding_file_path,\n",
    "         n_words=word_count,\n",
    "         k_neighbors=k,\n",
    "         threshold=threshold,\n",
    "         n_jobs=N_THREADS\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:22:49.355578Z",
     "start_time": "2019-03-29T01:22:20.625872Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix(7686, 7686)\n",
      "\n",
      "Cosine Filter: 0.9\n",
      "Computing TFIDF...\n",
      "Number of cluwords 7686\n",
      "Matrix(7686, 7686)\n",
      "\n",
      "Computing TF...\n",
      "Cluwords TF done in 1.245s.\n"
     ]
    }
   ],
   "source": [
    "cluwords = CluwordsTFIDF(dataset_file_path=dataset_file_path,\n",
    "                         n_words=word_count,\n",
    "                         cossine_filter=cossine_filter,\n",
    "                         path_to_save_cluwords=path_to_save_results,\n",
    "                         class_file_path=CLASS_PATH,\n",
    "                         has_class=HAS_CLASS)\n",
    "print('Computing TFIDF...')\n",
    "cluwords_tfidf = cluwords.fit_transform()\n",
    "# Convert the cluwords_tfidf array matrix to a sparse cluwords\n",
    "cluwords_tfidf = csr_matrix(cluwords_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-29T01:28:19.742938Z",
     "start_time": "2019-03-29T01:22:55.290906Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2374 and n_features=7686...\n",
      "W: (2374, 10) H:(7686, 10)\n",
      "results/artigos\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"\\nFitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\" % (cluwords.n_documents, cluwords.n_cluwords))\n",
    "\n",
    "nmf = NMF(n_components=N_COMPONENTS,\n",
    "          random_state=1,\n",
    "          alpha=.1,\n",
    "          l1_ratio=.5).fit(cluwords_tfidf)\n",
    "\n",
    "\n",
    "with open('{}/matrix_w.txt'.format(path_to_save_results), 'w') as f:\n",
    "    w = nmf.fit_transform(cluwords_tfidf)  # matrix W = m x k\n",
    "    h = nmf.components_.transpose()  # matrix H = n x k\n",
    "    print('W: {} H:{}'.format(w.shape, h.shape))\n",
    "    for x in range(w.shape[0]):\n",
    "        for y in range(w.shape[1]):\n",
    "            f.write('{} '.format(w[x][y]))\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "tfidf_feature_names = list(cluwords.vocab_cluwords)\n",
    "\n",
    "# -----------------------------\n",
    "# Avaliação dos resultados\n",
    "# Os resultados serao salvos na pasta results (nesse caso)\n",
    "n_cluwords, cluwords_vocab, cluwords_freq, cluwords_docs = Evaluation.count_tf_idf_repr(\n",
    "   cluwords.vocab_cluwords, cluwords_tfidf)\n",
    "\n",
    "# print('n_terms: {}'.format(n_cluwords))\n",
    "# print('words1: {}'.format(cluwords_vocab))\n",
    "# print('word_frequency: {}'.format(cluwords_freq))\n",
    "# print('term_docs: {}'.format(cluwords_docs))\n",
    "\n",
    "print_results(model=nmf,\n",
    "              tfidf_feature_names=tfidf_feature_names,\n",
    "              cluwords_freq=cluwords_freq,\n",
    "              cluwords_docs=cluwords_docs,\n",
    "              dataset=DATASET,\n",
    "              path_to_save_results=path_to_save_results,\n",
    "              path_to_save_model=PATH_TO_SAVE_MODEL\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "216px",
    "left": "1195px",
    "right": "20px",
    "top": "157px",
    "width": "361px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
