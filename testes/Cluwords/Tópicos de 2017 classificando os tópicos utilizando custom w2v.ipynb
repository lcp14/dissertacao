{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T18:47:40.636774Z",
     "start_time": "2019-03-20T18:47:40.633372Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/home/leandror/cluwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:28:23.935183Z",
     "start_time": "2019-03-21T01:28:23.856009Z"
    }
   },
   "outputs": [],
   "source": [
    "import cluwords\n",
    "from cluwords import Cluwords, CluwordsTFIDF\n",
    "import embedding\n",
    "from embedding import CreateEmbeddingModels\n",
    "from metrics import Evaluation\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:28:35.535780Z",
     "start_time": "2019-03-21T02:28:35.528733Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_embedding_models(dataset, embedding_file_path, embedding_type,\n",
    "                            datasets_path, path_to_save_model):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Create the word2vec models for each dataset\n",
    "    \"\"\"\n",
    "    word2vec_models = CreateEmbeddingModels(embedding_file_path=embedding_file_path,\n",
    "                                            embedding_type=embedding_type,\n",
    "                                            document_path=datasets_path,\n",
    "                                            path_to_save_model=path_to_save_model)\n",
    "    n_words = create_embedding_models(dataset)\n",
    "\n",
    "    return n_words\n",
    "def top_words(model, feature_names, n_top_words):\n",
    "    topico = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top = ''\n",
    "        top2 = ''\n",
    "        top += ' '.join([feature_names[i]\n",
    "                         for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        top2 += ''.join(str(sorted(topic)[:-n_top_words - 1:-1]))\n",
    "\n",
    "        topico.append(str(top))\n",
    "\n",
    "    return topico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:28:07.479121Z",
     "start_time": "2019-03-21T01:28:07.462745Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_results(model, tfidf_feature_names, cluwords_freq, cluwords_docs,\n",
    "                  dataset, path_to_save_results, path_to_save_model):\n",
    "    print(path_to_save_results)\n",
    "    for t in [5, 10, 20]:\n",
    "        with open('{}/result_topic_{}.txt'.format(path_to_save_results, t), 'w') as f_res:\n",
    "            f_res.write('Topics {}\\n'.format(t))\n",
    "            topics = top_words(model, tfidf_feature_names, t)\n",
    "            f_res.write('{}\\n'.format(topics))\n",
    "\n",
    "            coherence = Evaluation.coherence(topics, cluwords_freq, cluwords_docs)\n",
    "            f_res.write('Coherence: {} ({})\\n'.format(np.round(np.mean(coherence), 4), np.round(np.std(coherence), 4)))\n",
    "            f_res.write('{}\\n'.format(coherence))\n",
    "\n",
    "            pmi, npmi = Evaluation.pmi(topics, cluwords_freq, cluwords_docs,\n",
    "                                       sum([freq for word, freq in cluwords_freq.items()]), t)\n",
    "            f_res.write('PMI: {} ({})\\n'.format(np.round(np.mean(pmi), 4), np.round(np.std(pmi), 4)))\n",
    "            f_res.write('{}\\n'.format(pmi))\n",
    "            f_res.write('NPMI: {} ({})\\n'.format(np.round(np.mean(npmi), 4), np.round(np.std(npmi), 4)))\n",
    "            f_res.write('{}\\n'.format(npmi))\n",
    "\n",
    "            w2v_l1 = Evaluation.w2v_metric(topics, t, path_to_save_model, 'l1_dist', dataset)\n",
    "            f_res.write('W2V-L1: {} ({})\\n'.format(np.round(np.mean(w2v_l1), 4), np.round(np.std(w2v_l1), 4)))\n",
    "            f_res.write('{}\\n'.format(w2v_l1))\n",
    "\n",
    "            f_res.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:39.040480Z",
     "start_time": "2019-03-21T02:35:39.036328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variaveis que voce deve alterar:\n",
    "DATASETS_PATH = \"\"\"datasets\"\"\"\n",
    "PATH_TO_SAVE_RESULTS = \"\"\"results\"\"\"\n",
    "PATH_TO_SAVE_MODEL = \"\"\"word_emb_models/dataset_artigos_models\"\"\"\n",
    "EMBEDDINGS_FILE_PATH = \"\"\"word_emb_models/l2v.vec\"\"\"\n",
    "DATASET = \"artigos\"\n",
    "N_THREADS = 4\n",
    "N_COMPONENTS = 10\n",
    "\n",
    "# Nao precisa alterar essas:\n",
    "HAS_CLASS = False\n",
    "CLASS_PATH = \"\"\"\"\"\"\n",
    "EMBEDDINGS_BIN_TYPE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:46.028661Z",
     "start_time": "2019-03-21T02:35:44.607049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model read in 1.354s.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_embedding_models() missing 4 required positional arguments: 'embedding_file_path', 'embedding_type', 'datasets_path', and 'path_to_save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-96a9a810f9be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   \u001b[0membedding_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDINGS_BIN_TYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   \u001b[0mdatasets_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATASETS_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                   path_to_save_model=PATH_TO_SAVE_MODEL)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-036e0367d984>\u001b[0m in \u001b[0;36mcreate_embedding_models\u001b[0;34m(dataset, embedding_file_path, embedding_type, datasets_path, path_to_save_model)\u001b[0m\n\u001b[1;32m     10\u001b[0m                                             \u001b[0mdocument_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                             path_to_save_model=path_to_save_model)\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mn_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_embedding_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_embedding_models() missing 4 required positional arguments: 'embedding_file_path', 'embedding_type', 'datasets_path', and 'path_to_save_model'"
     ]
    }
   ],
   "source": [
    "# RUN ONE TIME\n",
    "n_words = create_embedding_models(dataset=DATASET,\n",
    "                                  embedding_file_path=EMBEDDINGS_FILE_PATH,\n",
    "                                  embedding_type=EMBEDDINGS_BIN_TYPE,\n",
    "                                  datasets_path=DATASETS_PATH,\n",
    "                                  path_to_save_model=PATH_TO_SAVE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:23.923490Z",
     "start_time": "2019-03-21T02:35:18.103Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "cossine_filter = 0.8\n",
    "word_count = n_words\n",
    "k = n_words\n",
    "algorithm_type = \"knn_cosine\"\n",
    "embedding_file_path = \"\"\"{}/{}.txt\"\"\".format(PATH_TO_SAVE_MODEL, DATASET)\n",
    "dataset_file_path = \"\"\"{}/{}Pre.txt\"\"\".format(DATASETS_PATH, DATASET)\n",
    "path_to_save_results = '{}/{}'.format(PATH_TO_SAVE_RESULTS, DATASET)\n",
    "\n",
    "try:\n",
    "    os.mkdir('{}'.format(path_to_save_results))\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:23.926470Z",
     "start_time": "2019-03-21T02:35:18.533Z"
    }
   },
   "outputs": [],
   "source": [
    "# Codigo das Cluwords\n",
    "Cluwords(algorithm=algorithm_type,\n",
    "         embedding_file_path=embedding_file_path,\n",
    "         n_words=word_count,\n",
    "         k_neighbors=k,\n",
    "         threshold=threshold,\n",
    "         n_jobs=N_THREADS\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:23.929507Z",
     "start_time": "2019-03-21T02:35:19.765Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "cluwords = CluwordsTFIDF(dataset_file_path=dataset_file_path,\n",
    "                         n_words=word_count,\n",
    "                         cossine_filter=cossine_filter,\n",
    "                         path_to_save_cluwords=path_to_save_results,\n",
    "                         class_file_path=CLASS_PATH,\n",
    "                         has_class=HAS_CLASS)\n",
    "print('Computing TFIDF...')\n",
    "cluwords_tfidf = cluwords.fit_transform()\n",
    "# Convert the cluwords_tfidf array matrix to a sparse cluwords\n",
    "cluwords_tfidf = csr_matrix(cluwords_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:23.932420Z",
     "start_time": "2019-03-21T02:35:20.252Z"
    }
   },
   "outputs": [],
   "source": [
    "# Fit the NMF model\n",
    "print(\"\\nFitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\" % (cluwords.n_documents, cluwords.n_cluwords))\n",
    "\n",
    "nmf = NMF(n_components=N_COMPONENTS,\n",
    "          random_state=1,\n",
    "          alpha=.1,\n",
    "          l1_ratio=.5).fit(cluwords_tfidf)\n",
    "\n",
    "\n",
    "with open('{}/matrix_w.txt'.format(path_to_save_results), 'w') as f:\n",
    "    w = nmf.fit_transform(cluwords_tfidf)  # matrix W = m x k\n",
    "    h = nmf.components_.transpose()  # matrix H = n x k\n",
    "    print('W: {} H:{}'.format(w.shape, h.shape))\n",
    "    for x in range(w.shape[0]):\n",
    "        for y in range(w.shape[1]):\n",
    "            f.write('{} '.format(w[x][y]))\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "tfidf_feature_names = list(cluwords.vocab_cluwords)\n",
    "\n",
    "# -----------------------------\n",
    "# Avaliação dos resultados\n",
    "# Os resultados serao salvos na pasta results (nesse caso)\n",
    "n_cluwords, cluwords_vocab, cluwords_freq, cluwords_docs = Evaluation.count_tf_idf_repr(\n",
    "    cluwords.vocab_cluwords, cluwords_tfidf)\n",
    "\n",
    "# print('n_terms: {}'.format(n_cluwords))\n",
    "# print('words1: {}'.format(cluwords_vocab))\n",
    "# print('word_frequency: {}'.format(cluwords_freq))\n",
    "# print('term_docs: {}'.format(cluwords_docs))\n",
    "\n",
    "print_results(model=nmf,\n",
    "              tfidf_feature_names=tfidf_feature_names,\n",
    "              cluwords_freq=cluwords_freq,\n",
    "              cluwords_docs=cluwords_docs,\n",
    "              dataset=DATASET,\n",
    "              path_to_save_results=path_to_save_results,\n",
    "              path_to_save_model=PATH_TO_SAVE_MODEL\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
