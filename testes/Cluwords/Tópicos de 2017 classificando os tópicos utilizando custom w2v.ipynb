{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T02:37:26.724620Z",
     "start_time": "2019-03-28T02:37:26.717549Z"
    }
   },
   "outputs": [],
   "source": [
    "#open('datasets/artigosPre.txt','w').write((\"\\n\".join(open('/home/leandror/new_corpus.txt','r').read().split('\\n')[17236:19610])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T02:37:26.773402Z",
     "start_time": "2019-03-28T02:37:26.728089Z"
=======
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-20T18:47:40.636774Z",
     "start_time": "2019-03-20T18:47:40.633372Z"
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'/home/leandror/cluwords')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T02:37:26.829844Z",
     "start_time": "2019-03-28T02:37:26.775669Z"
=======
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:28:23.935183Z",
     "start_time": "2019-03-21T01:28:23.856009Z"
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
    }
   },
   "outputs": [],
   "source": [
    "import cluwords\n",
    "from cluwords import Cluwords, CluwordsTFIDF\n",
    "import embedding\n",
    "from embedding import CreateEmbeddingModels\n",
    "from metrics import Evaluation\n",
    "from sklearn.decomposition import NMF\n",
    "from scipy.sparse import csr_matrix\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T02:37:26.880381Z",
     "start_time": "2019-03-28T02:37:26.834792Z"
=======
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:28:35.535780Z",
     "start_time": "2019-03-21T02:28:35.528733Z"
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
    }
   },
   "outputs": [],
   "source": [
    "def create_embedding_models(dataset, embedding_file_path, embedding_type,\n",
    "                            datasets_path, path_to_save_model):\n",
    "    \"\"\"\n",
    "    Description\n",
    "    -----------\n",
    "    Create the word2vec models for each dataset\n",
    "    \"\"\"\n",
    "    word2vec_models = CreateEmbeddingModels(embedding_file_path=embedding_file_path,\n",
    "                                            embedding_type=embedding_type,\n",
    "                                            document_path=datasets_path,\n",
    "                                            path_to_save_model=path_to_save_model)\n",
<<<<<<< HEAD
    "    n_words = word2vec_models.create_embedding_models(dataset)\n",
=======
    "    n_words = create_embedding_models(dataset)\n",
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
    "\n",
    "    return n_words\n",
    "def top_words(model, feature_names, n_top_words):\n",
    "    topico = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top = ''\n",
    "        top2 = ''\n",
    "        top += ' '.join([feature_names[i]\n",
    "                         for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        top2 += ''.join(str(sorted(topic)[:-n_top_words - 1:-1]))\n",
    "\n",
    "        topico.append(str(top))\n",
    "\n",
    "    return topico\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T02:41:12.590338Z",
     "start_time": "2019-03-28T02:41:12.571784Z"
=======
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T01:28:07.479121Z",
     "start_time": "2019-03-21T01:28:07.462745Z"
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
    }
   },
   "outputs": [],
   "source": [
    "def print_results(model, tfidf_feature_names, cluwords_freq, cluwords_docs,\n",
    "                  dataset, path_to_save_results, path_to_save_model):\n",
    "    print(path_to_save_results)\n",
    "    for t in [5, 10, 20]:\n",
    "        with open('{}/result_topic_{}.txt'.format(path_to_save_results, t), 'w') as f_res:\n",
    "            f_res.write('Topics {}\\n'.format(t))\n",
    "            topics = top_words(model, tfidf_feature_names, t)\n",
    "            f_res.write('{}\\n'.format(topics))\n",
    "\n",
    "            coherence = Evaluation.coherence(topics, cluwords_freq, cluwords_docs)\n",
    "            f_res.write('Coherence: {} ({})\\n'.format(np.round(np.mean(coherence), 4), np.round(np.std(coherence), 4)))\n",
    "            f_res.write('{}\\n'.format(coherence))\n",
    "\n",
    "            pmi, npmi = Evaluation.pmi(topics, cluwords_freq, cluwords_docs,\n",
    "                                       sum([freq for word, freq in cluwords_freq.items()]), t)\n",
    "            f_res.write('PMI: {} ({})\\n'.format(np.round(np.mean(pmi), 4), np.round(np.std(pmi), 4)))\n",
    "            f_res.write('{}\\n'.format(pmi))\n",
    "            f_res.write('NPMI: {} ({})\\n'.format(np.round(np.mean(npmi), 4), np.round(np.std(npmi), 4)))\n",
    "            f_res.write('{}\\n'.format(npmi))\n",
    "\n",
    "            w2v_l1 = Evaluation.w2v_metric(topics, t, path_to_save_model, 'l1_dist', dataset)\n",
    "            f_res.write('W2V-L1: {} ({})\\n'.format(np.round(np.mean(w2v_l1), 4), np.round(np.std(w2v_l1), 4)))\n",
    "            f_res.write('{}\\n'.format(w2v_l1))\n",
    "\n",
    "            f_res.close()"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T02:41:21.529335Z",
     "start_time": "2019-03-28T02:41:21.525935Z"
=======
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:39.040480Z",
     "start_time": "2019-03-21T02:35:39.036328Z"
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
    }
   },
   "outputs": [],
   "source": [
    "# Variaveis que voce deve alterar:\n",
    "DATASETS_PATH = \"\"\"datasets\"\"\"\n",
    "PATH_TO_SAVE_RESULTS = \"\"\"results\"\"\"\n",
    "PATH_TO_SAVE_MODEL = \"\"\"word_emb_models/dataset_artigos_models\"\"\"\n",
    "EMBEDDINGS_FILE_PATH = \"\"\"word_emb_models/l2v.vec\"\"\"\n",
    "DATASET = \"artigos\"\n",
    "N_THREADS = 4\n",
    "N_COMPONENTS = 10\n",
    "\n",
    "# Nao precisa alterar essas:\n",
    "HAS_CLASS = False\n",
    "CLASS_PATH = \"\"\"\"\"\"\n",
    "EMBEDDINGS_BIN_TYPE = False"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T02:42:23.488007Z",
     "start_time": "2019-03-28T02:42:20.157010Z"
=======
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:46.028661Z",
     "start_time": "2019-03-21T02:35:44.607049Z"
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Embedding model read in 2.494s.\n",
      "artigos: 3930\n"
=======
      "Embedding model read in 1.354s.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_embedding_models() missing 4 required positional arguments: 'embedding_file_path', 'embedding_type', 'datasets_path', and 'path_to_save_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-96a9a810f9be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                   \u001b[0membedding_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEMBEDDINGS_BIN_TYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                   \u001b[0mdatasets_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATASETS_PATH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                   path_to_save_model=PATH_TO_SAVE_MODEL)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-77-036e0367d984>\u001b[0m in \u001b[0;36mcreate_embedding_models\u001b[0;34m(dataset, embedding_file_path, embedding_type, datasets_path, path_to_save_model)\u001b[0m\n\u001b[1;32m     10\u001b[0m                                             \u001b[0mdocument_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatasets_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m                                             path_to_save_model=path_to_save_model)\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mn_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_embedding_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mn_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: create_embedding_models() missing 4 required positional arguments: 'embedding_file_path', 'embedding_type', 'datasets_path', and 'path_to_save_model'"
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "# # RUN ONE TIME\n",
=======
    "# RUN ONE TIME\n",
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
    "n_words = create_embedding_models(dataset=DATASET,\n",
    "                                  embedding_file_path=EMBEDDINGS_FILE_PATH,\n",
    "                                  embedding_type=EMBEDDINGS_BIN_TYPE,\n",
    "                                  datasets_path=DATASETS_PATH,\n",
    "                                  path_to_save_model=PATH_TO_SAVE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T02:42:44.397323Z",
     "start_time": "2019-03-28T02:42:44.387800Z"
=======
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:23.923490Z",
     "start_time": "2019-03-21T02:35:18.103Z"
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
    }
   },
   "outputs": [],
   "source": [
    "threshold = 0.4\n",
    "cossine_filter = 0.8\n",
    "word_count = n_words\n",
    "k = n_words\n",
    "algorithm_type = \"knn_cosine\"\n",
    "embedding_file_path = \"\"\"{}/{}.txt\"\"\".format(PATH_TO_SAVE_MODEL, DATASET)\n",
    "dataset_file_path = \"\"\"{}/{}Pre.txt\"\"\".format(DATASETS_PATH, DATASET)\n",
    "path_to_save_results = '{}/{}'.format(PATH_TO_SAVE_RESULTS, DATASET)\n",
<<<<<<< HEAD
    "import os\n",
=======
    "\n",
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
    "try:\n",
    "    os.mkdir('{}'.format(path_to_save_results))\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T02:46:14.292021Z",
     "start_time": "2019-03-28T02:42:52.206815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kNN...\n",
      "N Threads: 4\n",
      "NearestNeighbors K=3930\n",
      "Time 0.0005443450063467026\n",
      "NN Distaces\n",
      "Time 2.3504377540084533\n",
      "Saving cluwords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<cluwords.Cluwords at 0x7fc31cc78940>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:23.926470Z",
     "start_time": "2019-03-21T02:35:18.533Z"
    }
   },
   "outputs": [],
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
   "source": [
    "# Codigo das Cluwords\n",
    "Cluwords(algorithm=algorithm_type,\n",
    "         embedding_file_path=embedding_file_path,\n",
    "         n_words=word_count,\n",
    "         k_neighbors=k,\n",
    "         threshold=threshold,\n",
    "         n_jobs=N_THREADS\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T02:51:11.796703Z",
     "start_time": "2019-03-28T02:50:42.076199Z"
    },
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix(3930, 3930)\n",
      "\n",
      "Cosine Filter: 0.8\n",
      "Computing TFIDF...\n",
      "Number of cluwords 3930\n",
      "Matrix(3930, 3930)\n",
      "\n",
      "Computing TF...\n",
      "Cluwords TF done in 0.689s.\n"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:23.929507Z",
     "start_time": "2019-03-21T02:35:19.765Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
   "source": [
    "cluwords = CluwordsTFIDF(dataset_file_path=dataset_file_path,\n",
    "                         n_words=word_count,\n",
    "                         cossine_filter=cossine_filter,\n",
    "                         path_to_save_cluwords=path_to_save_results,\n",
    "                         class_file_path=CLASS_PATH,\n",
    "                         has_class=HAS_CLASS)\n",
    "print('Computing TFIDF...')\n",
    "cluwords_tfidf = cluwords.fit_transform()\n",
    "# Convert the cluwords_tfidf array matrix to a sparse cluwords\n",
    "cluwords_tfidf = csr_matrix(cluwords_tfidf)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T02:55:18.637784Z",
     "start_time": "2019-03-28T02:51:40.323244Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=2374 and n_features=3930...\n",
      "W: (2374, 10) H:(3930, 10)\n",
      "results/artigos\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid vector on line 0 (is this really the text format?)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-0d358f2f8e09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m               \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDATASET\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m               \u001b[0mpath_to_save_results\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_to_save_results\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m               \u001b[0mpath_to_save_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPATH_TO_SAVE_MODEL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m               )\n",
      "\u001b[0;32m<ipython-input-50-71c407ebf421>\u001b[0m in \u001b[0;36mprint_results\u001b[0;34m(model, tfidf_feature_names, cluwords_freq, cluwords_docs, dataset, path_to_save_results, path_to_save_model)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mf_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnpmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mw2v_l1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2v_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_save_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'l1_dist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mf_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'W2V-L1: {} ({})\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_l1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_l1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mf_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_l1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cluwords/metrics.py\u001b[0m in \u001b[0;36mw2v_metric\u001b[0;34m(topics, t, path_to_save_model, distance_type, dataset, embedding_type)\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mw2v_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_to_save_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistance_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         word_vectors = KeyedVectors.load_word2vec_format(\n\u001b[0;32m--> 184\u001b[0;31m             fname='{}/{}.txt'.format(path_to_save_model, dataset), binary=embedding_type)\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1474\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1475\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1476\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1478\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    392\u001b[0m                 \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"invalid vector on line %s (is this really the text format?)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mline_no\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m                 \u001b[0madd_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid vector on line 0 (is this really the text format?)"
     ]
    }
   ],
=======
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-21T02:35:23.932420Z",
     "start_time": "2019-03-21T02:35:20.252Z"
    }
   },
   "outputs": [],
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
   "source": [
    "# Fit the NMF model\n",
    "print(\"\\nFitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\" % (cluwords.n_documents, cluwords.n_cluwords))\n",
    "\n",
    "nmf = NMF(n_components=N_COMPONENTS,\n",
    "          random_state=1,\n",
    "          alpha=.1,\n",
    "          l1_ratio=.5).fit(cluwords_tfidf)\n",
    "\n",
    "\n",
    "with open('{}/matrix_w.txt'.format(path_to_save_results), 'w') as f:\n",
    "    w = nmf.fit_transform(cluwords_tfidf)  # matrix W = m x k\n",
    "    h = nmf.components_.transpose()  # matrix H = n x k\n",
    "    print('W: {} H:{}'.format(w.shape, h.shape))\n",
    "    for x in range(w.shape[0]):\n",
    "        for y in range(w.shape[1]):\n",
    "            f.write('{} '.format(w[x][y]))\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "tfidf_feature_names = list(cluwords.vocab_cluwords)\n",
    "\n",
    "# -----------------------------\n",
    "# Avaliação dos resultados\n",
    "# Os resultados serao salvos na pasta results (nesse caso)\n",
    "n_cluwords, cluwords_vocab, cluwords_freq, cluwords_docs = Evaluation.count_tf_idf_repr(\n",
    "    cluwords.vocab_cluwords, cluwords_tfidf)\n",
    "\n",
    "# print('n_terms: {}'.format(n_cluwords))\n",
    "# print('words1: {}'.format(cluwords_vocab))\n",
    "# print('word_frequency: {}'.format(cluwords_freq))\n",
    "# print('term_docs: {}'.format(cluwords_docs))\n",
    "\n",
    "print_results(model=nmf,\n",
    "              tfidf_feature_names=tfidf_feature_names,\n",
    "              cluwords_freq=cluwords_freq,\n",
    "              cluwords_docs=cluwords_docs,\n",
    "              dataset=DATASET,\n",
    "              path_to_save_results=path_to_save_results,\n",
    "              path_to_save_model=PATH_TO_SAVE_MODEL\n",
    "              )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
<<<<<<< HEAD
   "position": {
    "height": "144px",
    "left": "1551px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
=======
>>>>>>> 236307e68e7570ff27e8402eead9ca0f3459820e
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
