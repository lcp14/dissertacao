{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré Processamento de um conjunto de artigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolher uma base de artigos e realizar o pré-processamento, sem utilizar a biblioteca Spacy para ajudar na remoção de palavras.\n",
    "\n",
    "O pré processamento se resume em retirar stop words, normalizar o texto, remover números, e remover ruidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:03:56.555567Z",
     "start_time": "2019-03-15T18:03:47.075868Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leandro/anaconda3/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "# Bibliotecas nativas\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile\n",
    "import pickle\n",
    "# Pré processamento e estruturas de dados\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from langdetect import DetectorFactory, detect\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "import enchant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:03:59.645452Z",
     "start_time": "2019-03-15T18:03:56.581334Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en',disable=['parser','ner'])\n",
    "nlp_pt = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:03:59.662960Z",
     "start_time": "2019-03-15T18:03:59.659943Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:03:59.774027Z",
     "start_time": "2019-03-15T18:03:59.675286Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "def detect_portuguese_text(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "    except:\n",
    "        return np.nan\n",
    "    if language == 'en':\n",
    "        return text\n",
    "    else:\n",
    "        return np.nan\n",
    "def remove_portuguese_text(text):\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    \n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(text)\n",
    "                    if w.lower() in words or not w.isalpha())\n",
    "def remove_useless_words(text):\n",
    "    doc = nlp(text)\n",
    "    text = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['VERB'] and not token.is_stop:\n",
    "            text += token.lemma_ + \" \"\n",
    "    return text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "#     text = remove_portuguese_text(text)\n",
    "#     text = remove_useless_words(text)\n",
    "#     text = detect_portuguese_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:03:59.835315Z",
     "start_time": "2019-03-15T18:03:59.785352Z"
    },
    "code_folding": [
     9
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    \n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word.lemma_).encode(\n",
    "            'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "#def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lemma_.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "def remove_number(words):\n",
    "    text = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            if not word.isdigit():\n",
    "                text.append(word)\n",
    "        except ValueError:\n",
    "            print(word)\n",
    "    return text    \n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    #words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = remove_number(words)\n",
    "#     words = replace_numbers(words)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:04:00.016004Z",
     "start_time": "2019-03-15T18:03:59.840995Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_small_words(wordst):\n",
    "    new_words = []\n",
    "    for word in wordst:\n",
    "        if len(word) > 1:\n",
    "            #print(word)\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "def remove_useless_words(text):\n",
    "    doc = nlp(text)\n",
    "    text = \"\"\n",
    "    lemma = ['ADJ','NOUN']\n",
    "    for token in doc:\n",
    "            if token.pos_ not in ['ADJ']:\n",
    "                text += token.lemma_ + \" \"\n",
    "    return text\n",
    "def remove_stopwords_pt(wordst):\n",
    "    stop = stopwords.words('portuguese')\n",
    "    stop = remove_non_ascii(stop)\n",
    "    new_words = [w for w in wordst if w not in stop]\n",
    "\n",
    "    return new_words\n",
    "def remove_stopwords_en(wordst):\n",
    "    stop = stopwords.words('english')\n",
    "    stop = remove_non_ascii(stop)\n",
    "    new_words = [w for w in wordst if w not in stop]\n",
    "\n",
    "    return new_words\n",
    "\n",
    "d = enchant.Dict('en_US')\n",
    "def remove_portuguese(words):\n",
    "    english_words = []\n",
    "    totalLength = len(words)\n",
    "    cont = 0\n",
    "    for word in words:\n",
    "        word = word.lemma_\n",
    "        if d.check(word):\n",
    "            english_words.append(word)\n",
    "        else:\n",
    "            cont+= 1\n",
    "   # print(len(english_words),totalLength)\n",
    "    if totalLength > 0:        \n",
    "        rate = cont/totalLength\n",
    "    else: \n",
    "        rate = 0\n",
    "    #print(rate)\n",
    "    if (rate) > 0.2:\n",
    "        new_words = []\n",
    "        return new_words\n",
    "    else:\n",
    "        return english_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:28:48.533893Z",
     "start_time": "2019-03-15T18:28:48.530298Z"
    }
   },
   "outputs": [],
   "source": [
    "year = '2014'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:28:48.841923Z",
     "start_time": "2019-03-15T18:28:48.698618Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leandro/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>autores</th>\n",
       "      <th>titulo</th>\n",
       "      <th>ano</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Helena Barreto Matzenauer&amp;Wiliam Correa Marque...</td>\n",
       "      <td>A operacionalidade do modelo multicriterio de ...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernard Gendron&amp;Abilio Lucena&amp;Alexandre Salles...</td>\n",
       "      <td>Benders Decomposition, Branch-and-Cut, and Hyb...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>João dos Santos Vila da Silva&amp;Sandra Mara Alve...</td>\n",
       "      <td>Caracterizacao ambiental da unidade de planeja...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Felype Santiago&amp;Rohit Gheyi&amp;Gustavo Soares&amp;Pau...</td>\n",
       "      <td>A Toolset for Checking SPL Refinements</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WANICK SARINHO, SILVIA&amp;MENEZES JÚNIOR, JÚLIO V...</td>\n",
       "      <td>Experience In The Development Of a Mobile Diag...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nemésio Freitas Duarte Filho&amp;Lucas Bortolini F...</td>\n",
       "      <td>Contributions for the Architectural Design of ...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UEYAMA, JO&amp;FREITAS, HEITOR&amp;FAICAL, BRUNO S.&amp;FI...</td>\n",
       "      <td>Exploiting the use of unmanned aerial vehicles...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Luiz Olmes de Carvalho&amp;Willian Dener de Olivei...</td>\n",
       "      <td>A wider concept for similarity joins</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CUADRAT, RAFAEL R. C.&amp;DÁVILA, ALBERTO M. R.&amp;CR...</td>\n",
       "      <td>An Orthology-Based Analysis of Pathogenic Prot...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Fabricio Herpich&amp;Rafaela Ribeiro Jardim&amp;Ricard...</td>\n",
       "      <td>CYBERCIEGE: uma abordagem de jogos serios na e...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             autores  \\\n",
       "0  Helena Barreto Matzenauer&Wiliam Correa Marque...   \n",
       "1  Bernard Gendron&Abilio Lucena&Alexandre Salles...   \n",
       "2  João dos Santos Vila da Silva&Sandra Mara Alve...   \n",
       "3  Felype Santiago&Rohit Gheyi&Gustavo Soares&Pau...   \n",
       "4  WANICK SARINHO, SILVIA&MENEZES JÚNIOR, JÚLIO V...   \n",
       "5  Nemésio Freitas Duarte Filho&Lucas Bortolini F...   \n",
       "6  UEYAMA, JO&FREITAS, HEITOR&FAICAL, BRUNO S.&FI...   \n",
       "7  Luiz Olmes de Carvalho&Willian Dener de Olivei...   \n",
       "8  CUADRAT, RAFAEL R. C.&DÁVILA, ALBERTO M. R.&CR...   \n",
       "9  Fabricio Herpich&Rafaela Ribeiro Jardim&Ricard...   \n",
       "\n",
       "                                              titulo     ano  \n",
       "0  A operacionalidade do modelo multicriterio de ...  2014.0  \n",
       "1  Benders Decomposition, Branch-and-Cut, and Hyb...  2014.0  \n",
       "2  Caracterizacao ambiental da unidade de planeja...  2014.0  \n",
       "3             A Toolset for Checking SPL Refinements  2014.0  \n",
       "4  Experience In The Development Of a Mobile Diag...  2014.0  \n",
       "5  Contributions for the Architectural Design of ...  2014.0  \n",
       "6  Exploiting the use of unmanned aerial vehicles...  2014.0  \n",
       "7               A wider concept for similarity joins  2014.0  \n",
       "8  An Orthology-Based Analysis of Pathogenic Prot...  2014.0  \n",
       "9  CYBERCIEGE: uma abordagem de jogos serios na e...  2014.0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_artigos = pd.read_csv('../datasets/artigos/nopreprocessed/artigos_'+year+'.csv',sep=\"\\*\\|\\*\")\n",
    "df_artigos.dropna(inplace=True)\n",
    "df_artigos = df_artigos.reset_index(drop=True)\n",
    "\n",
    "df_artigos = df_artigos.sample(\n",
    "    frac=1, random_state=29).reset_index(drop=True)  # Embaralha (shuffle) as linhas\n",
    "\n",
    "df_artigos.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:28:48.875620Z",
     "start_time": "2019-03-15T18:28:48.871902Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_pre = df_artigos.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T16:43:55.324755Z",
     "start_time": "2019-02-01T16:43:55.312922Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:28:49.178718Z",
     "start_time": "2019-03-15T18:28:49.175420Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_pre['titulo'] = df_pre['titulo'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:05.036620Z",
     "start_time": "2019-03-15T18:28:49.327502Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(denoise_text)\n",
    "\n",
    "df_pre['titulo'] = df_pre['titulo'].apply(remove_useless_words)\n",
    "\n",
    "df_pre['titulo'] = df_pre['titulo'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:05.069756Z",
     "start_time": "2019-03-15T18:29:05.065732Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_normalize(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.is_ascii and not token.is_digit and not token.is_punct and not token.is_stop and token.is_alpha:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:05.205958Z",
     "start_time": "2019-03-15T18:29:05.098373Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(new_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:06.061877Z",
     "start_time": "2019-03-15T18:29:05.231594Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(remove_portuguese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:06.091879Z",
     "start_time": "2019-03-15T18:29:06.084978Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre = df_pre[df_pre['titulo'].apply(len) >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:06.203389Z",
     "start_time": "2019-03-15T18:29:06.115075Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:06.259030Z",
     "start_time": "2019-03-15T18:29:06.220256Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>autores</th>\n",
       "      <th>titulo</th>\n",
       "      <th>ano</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernard Gendron&amp;Abilio Lucena&amp;Alexandre Salles...</td>\n",
       "      <td>bender decomposition branch cut hybrid algorit...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WANICK SARINHO, SILVIA&amp;MENEZES JÚNIOR, JÚLIO V...</td>\n",
       "      <td>experience development mobile diagnosis suppor...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Nemésio Freitas Duarte Filho&amp;Lucas Bortolini F...</td>\n",
       "      <td>contribution architectural design mobile learn...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>UEYAMA, JO&amp;FREITAS, HEITOR&amp;FAICAL, BRUNO S.&amp;FI...</td>\n",
       "      <td>exploit use vehicle provide resilience sensor ...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Luiz Olmes de Carvalho&amp;Willian Dener de Olivei...</td>\n",
       "      <td>concept similarity join</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gilberto Câmara&amp;Max J. Egenhofer&amp;Karine Reis F...</td>\n",
       "      <td>field generic datum type big spatial datum</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Jeongyeon Seo&amp;Jae-Kwan Kim&amp;Joonghyun Ryu&amp;Carli...</td>\n",
       "      <td>protein structure determination algorithm base...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Fabiano Belem&amp;Eder Martins&amp;Jussara Marques Alm...</td>\n",
       "      <td>personalize object center tag recommendation m...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SOUZA SANTANA-VIEIRA, DAYSE DRIELLY&amp;PEREIRA MI...</td>\n",
       "      <td>rapid differentiation closely citrus fluoresce...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Antônio Vicente Lourenço Damaso&amp;Nelson Rosa&amp;Pa...</td>\n",
       "      <td>use net evaluate power consumption wireless se...</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              autores  \\\n",
       "1   Bernard Gendron&Abilio Lucena&Alexandre Salles...   \n",
       "4   WANICK SARINHO, SILVIA&MENEZES JÚNIOR, JÚLIO V...   \n",
       "5   Nemésio Freitas Duarte Filho&Lucas Bortolini F...   \n",
       "6   UEYAMA, JO&FREITAS, HEITOR&FAICAL, BRUNO S.&FI...   \n",
       "7   Luiz Olmes de Carvalho&Willian Dener de Olivei...   \n",
       "10  Gilberto Câmara&Max J. Egenhofer&Karine Reis F...   \n",
       "11  Jeongyeon Seo&Jae-Kwan Kim&Joonghyun Ryu&Carli...   \n",
       "12  Fabiano Belem&Eder Martins&Jussara Marques Alm...   \n",
       "13  SOUZA SANTANA-VIEIRA, DAYSE DRIELLY&PEREIRA MI...   \n",
       "19  Antônio Vicente Lourenço Damaso&Nelson Rosa&Pa...   \n",
       "\n",
       "                                               titulo     ano  \n",
       "1   bender decomposition branch cut hybrid algorit...  2014.0  \n",
       "4   experience development mobile diagnosis suppor...  2014.0  \n",
       "5   contribution architectural design mobile learn...  2014.0  \n",
       "6   exploit use vehicle provide resilience sensor ...  2014.0  \n",
       "7                             concept similarity join  2014.0  \n",
       "10         field generic datum type big spatial datum  2014.0  \n",
       "11  protein structure determination algorithm base...  2014.0  \n",
       "12  personalize object center tag recommendation m...  2014.0  \n",
       "13  rapid differentiation closely citrus fluoresce...  2014.0  \n",
       "19  use net evaluate power consumption wireless se...  2014.0  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:06.312444Z",
     "start_time": "2019-03-15T18:29:06.277203Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre = df_pre.drop_duplicates(subset=['titulo'],keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:06.361340Z",
     "start_time": "2019-03-15T18:29:06.331958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2184, 3)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:06.410939Z",
     "start_time": "2019-03-15T18:29:06.379803Z"
    }
   },
   "outputs": [],
   "source": [
    "artigos = df_pre['titulo'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:06.459784Z",
     "start_time": "2019-03-15T18:29:06.431470Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('../datasets/artigos/preprocessed/'+year+'/artigosPre.txt','w')\n",
    "f.write(\"\".join(artigos+\"\\n\"))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separar em grupos procurando artigos iguais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T18:29:06.524926Z",
     "start_time": "2019-03-15T18:29:06.481657Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre.to_csv('../datasets/artigos/preprocessed/'+year+'/artigosPre.csv',sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
