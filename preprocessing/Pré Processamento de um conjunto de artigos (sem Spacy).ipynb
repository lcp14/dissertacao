{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré Processamento de um conjunto de artigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolher uma base de artigos e realizar o pré-processamento, sem utilizar a biblioteca Spacy para ajudar na remoção de palavras.\n",
    "\n",
    "O pré processamento se resume em retirar stop words, normalizar o texto, remover números, e remover ruidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-03-15T14:57:17.107522Z",
     "start_time": "2019-03-15T14:57:16.455393Z"
=======
     "end_time": "2019-03-15T18:03:56.555567Z",
     "start_time": "2019-03-15T18:03:47.075868Z"
>>>>>>> c54f32da20c312374d6af59144d9a4c9f88a3c79
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'enchant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e0b12d554699>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'enchant'"
     ]
    }
   ],
   "source": [
    "# Bibliotecas nativas\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile\n",
    "import pickle\n",
    "# Pré processamento e estruturas de dados\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from langdetect import DetectorFactory, detect\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "import enchant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-03-15T14:52:36.838126Z",
     "start_time": "2019-03-15T14:52:35.282Z"
=======
     "end_time": "2019-03-15T18:03:59.645452Z",
     "start_time": "2019-03-15T18:03:56.581334Z"
>>>>>>> c54f32da20c312374d6af59144d9a4c9f88a3c79
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en',disable=['parser','ner'])\n",
    "nlp_pt = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-03-15T14:52:36.843768Z",
     "start_time": "2019-03-15T14:52:35.621Z"
=======
     "end_time": "2019-03-15T18:03:59.662960Z",
     "start_time": "2019-03-15T18:03:59.659943Z"
>>>>>>> c54f32da20c312374d6af59144d9a4c9f88a3c79
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-03-15T14:52:36.849580Z",
     "start_time": "2019-03-15T14:52:36.219Z"
=======
     "end_time": "2019-03-15T18:03:59.774027Z",
     "start_time": "2019-03-15T18:03:59.675286Z"
>>>>>>> c54f32da20c312374d6af59144d9a4c9f88a3c79
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "def detect_portuguese_text(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "    except:\n",
    "        return np.nan\n",
    "    if language == 'en':\n",
    "        return text\n",
    "    else:\n",
    "        return np.nan\n",
    "def remove_portuguese_text(text):\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    \n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(text)\n",
    "                    if w.lower() in words or not w.isalpha())\n",
    "def remove_useless_words(text):\n",
    "    doc = nlp(text)\n",
    "    text = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['VERB'] and not token.is_stop:\n",
    "            text += token.lemma_ + \" \"\n",
    "    return text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "#     text = remove_portuguese_text(text)\n",
    "#     text = remove_useless_words(text)\n",
    "#     text = detect_portuguese_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-03-15T14:52:36.854923Z",
     "start_time": "2019-03-15T14:52:36.720Z"
=======
     "end_time": "2019-03-15T18:03:59.835315Z",
     "start_time": "2019-03-15T18:03:59.785352Z"
>>>>>>> c54f32da20c312374d6af59144d9a4c9f88a3c79
    },
    "code_folding": [
     9
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    \n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word.lemma_).encode(\n",
    "            'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "#def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lemma_.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "def remove_number(words):\n",
    "    text = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            if not word.isdigit():\n",
    "                text.append(word)\n",
    "        except ValueError:\n",
    "            print(word)\n",
    "    return text    \n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    #words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = remove_number(words)\n",
    "#     words = replace_numbers(words)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
<<<<<<< HEAD
     "end_time": "2019-03-15T14:52:37.882519Z",
     "start_time": "2019-03-15T14:52:37.856207Z"
=======
     "end_time": "2019-03-15T18:04:00.016004Z",
     "start_time": "2019-03-15T18:03:59.840995Z"
>>>>>>> c54f32da20c312374d6af59144d9a4c9f88a3c79
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enchant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b02cca8a7689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_US'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_portuguese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0menglish_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enchant' is not defined"
     ]
    }
   ],
   "source": [
    "def remove_small_words(wordst):\n",
    "    new_words = []\n",
    "    for word in wordst:\n",
    "        if len(word) > 1:\n",
    "            #print(word)\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "def remove_useless_words(text):\n",
    "    doc = nlp(text)\n",
    "    text = \"\"\n",
    "    lemma = ['ADJ','NOUN']\n",
    "    for token in doc:\n",
    "            if token.pos_ not in ['ADJ']:\n",
    "                text += token.lemma_ + \" \"\n",
    "    return text\n",
    "def remove_stopwords_pt(wordst):\n",
    "    stop = stopwords.words('portuguese')\n",
    "    stop = remove_non_ascii(stop)\n",
    "    new_words = [w for w in wordst if w not in stop]\n",
    "\n",
    "    return new_words\n",
    "def remove_stopwords_en(wordst):\n",
    "    stop = stopwords.words('english')\n",
    "    stop = remove_non_ascii(stop)\n",
    "    new_words = [w for w in wordst if w not in stop]\n",
    "\n",
    "    return new_words\n",
    "\n",
    "d = enchant.Dict('en_US')\n",
    "def remove_portuguese(words):\n",
    "    english_words = []\n",
    "    totalLength = len(words)\n",
    "    cont = 0\n",
    "    for word in words:\n",
    "        word = word.lemma_\n",
    "        if d.check(word):\n",
    "            english_words.append(word)\n",
    "        else:\n",
    "            cont+= 1\n",
    "   # print(len(english_words),totalLength)\n",
    "    if totalLength > 0:        \n",
    "        rate = cont/totalLength\n",
    "    else: \n",
    "        rate = 0\n",
    "    #print(rate)\n",
    "    if (rate) > 0.2:\n",
    "        new_words = []\n",
    "        return new_words\n",
    "    else:\n",
    "        return english_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:22.473846Z",
     "start_time": "2019-03-15T19:06:22.469977Z"
    }
   },
   "outputs": [],
   "source": [
    "year = '2013'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:22.799229Z",
     "start_time": "2019-03-15T19:06:22.652591Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leandro/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>autores</th>\n",
       "      <th>titulo</th>\n",
       "      <th>ano</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>REBÊLO, HENRIQUE&amp;LIMA, RICARDO&amp;KULESZA, UIRÁ&amp;R...</td>\n",
       "      <td>QUANTIFYING THE EFFECTS OF ASPECTUAL DECOMPOSI...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rogério Galante Negri&amp;Carlos Alberto Pires de ...</td>\n",
       "      <td>At-Som: Mapas Auto-Organizaveis Atenuantes</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emerson Antonio Maccari&amp;Edson Luiz Riccio&amp;Cibe...</td>\n",
       "      <td>A influencia do sistema de avaliacao da AACSB ...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Egon Walter Wildauer&amp;Fabiana Hoffmann da Silva</td>\n",
       "      <td>Ethical, social, privacy, security and moral i...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deisy Cristina Corrêa Igarashi&amp;José Braz Herco...</td>\n",
       "      <td>Adocao de estrategias e o impacto sobre os ind...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Fábio Abrantes Diniz&amp;Francisco Milton Mendes N...</td>\n",
       "      <td>RedFace: um sistema de reconhecimento facial b...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Davide Vega&amp;Roc Meseguer&amp;Sergio Fabian Ochoa&amp;J...</td>\n",
       "      <td>Sharing Hardware Resources in Heterogeneous Co...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pauline Provini</td>\n",
       "      <td>Study on the pelvic system of birds and on the...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Eduardo Ogasawara&amp;Daniel Cardoso Moraes de Oli...</td>\n",
       "      <td>A Forecasting Method for Fertilizers Consumpti...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SLEIMAN, HASSAN A.&amp;Rafael Corchuelo</td>\n",
       "      <td>A Survey on Region Extractors from Web Documents</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             autores  \\\n",
       "0  REBÊLO, HENRIQUE&LIMA, RICARDO&KULESZA, UIRÁ&R...   \n",
       "1  Rogério Galante Negri&Carlos Alberto Pires de ...   \n",
       "2  Emerson Antonio Maccari&Edson Luiz Riccio&Cibe...   \n",
       "3     Egon Walter Wildauer&Fabiana Hoffmann da Silva   \n",
       "4  Deisy Cristina Corrêa Igarashi&José Braz Herco...   \n",
       "5  Fábio Abrantes Diniz&Francisco Milton Mendes N...   \n",
       "6  Davide Vega&Roc Meseguer&Sergio Fabian Ochoa&J...   \n",
       "7                                    Pauline Provini   \n",
       "8  Eduardo Ogasawara&Daniel Cardoso Moraes de Oli...   \n",
       "9                SLEIMAN, HASSAN A.&Rafael Corchuelo   \n",
       "\n",
       "                                              titulo   ano  \n",
       "0  QUANTIFYING THE EFFECTS OF ASPECTUAL DECOMPOSI...  2013  \n",
       "1         At-Som: Mapas Auto-Organizaveis Atenuantes  2013  \n",
       "2  A influencia do sistema de avaliacao da AACSB ...  2013  \n",
       "3  Ethical, social, privacy, security and moral i...  2013  \n",
       "4  Adocao de estrategias e o impacto sobre os ind...  2013  \n",
       "5  RedFace: um sistema de reconhecimento facial b...  2013  \n",
       "6  Sharing Hardware Resources in Heterogeneous Co...  2013  \n",
       "7  Study on the pelvic system of birds and on the...  2013  \n",
       "8  A Forecasting Method for Fertilizers Consumpti...  2013  \n",
       "9   A Survey on Region Extractors from Web Documents  2013  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_artigos = pd.read_csv('../datasets/artigos/nopreprocessed/artigos_'+year+'.csv',sep=\"\\*\\|\\*\")\n",
    "df_artigos.dropna(inplace=True)\n",
    "df_artigos = df_artigos.reset_index(drop=True)\n",
    "\n",
    "df_artigos = df_artigos.sample(\n",
    "    frac=1, random_state=29).reset_index(drop=True)  # Embaralha (shuffle) as linhas\n",
    "\n",
    "df_artigos.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:22.819088Z",
     "start_time": "2019-03-15T19:06:22.816385Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_pre = df_artigos.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T16:43:55.324755Z",
     "start_time": "2019-02-01T16:43:55.312922Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:23.056265Z",
     "start_time": "2019-03-15T19:06:23.052696Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_pre['titulo'] = df_pre['titulo'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:38.746653Z",
     "start_time": "2019-03-15T19:06:23.201581Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(denoise_text)\n",
    "\n",
    "df_pre['titulo'] = df_pre['titulo'].apply(remove_useless_words)\n",
    "\n",
    "df_pre['titulo'] = df_pre['titulo'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:38.776532Z",
     "start_time": "2019-03-15T19:06:38.773462Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_normalize(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.is_ascii and not token.is_digit and not token.is_punct and not token.is_stop and token.is_alpha:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:38.919573Z",
     "start_time": "2019-03-15T19:06:38.804664Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(new_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:39.742123Z",
     "start_time": "2019-03-15T19:06:38.943845Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(remove_portuguese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:39.769118Z",
     "start_time": "2019-03-15T19:06:39.764060Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre = df_pre[df_pre['titulo'].apply(len) >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:39.883005Z",
     "start_time": "2019-03-15T19:06:39.790788Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:39.935126Z",
     "start_time": "2019-03-15T19:06:39.901571Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>autores</th>\n",
       "      <th>titulo</th>\n",
       "      <th>ano</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Egon Walter Wildauer&amp;Fabiana Hoffmann da Silva</td>\n",
       "      <td>privacy security issue e society</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Davide Vega&amp;Roc Meseguer&amp;Sergio Fabian Ochoa&amp;J...</td>\n",
       "      <td>share hardware resource heterogeneous computer...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pauline Provini</td>\n",
       "      <td>study system bird origin flight</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Eduardo Ogasawara&amp;Daniel Cardoso Moraes de Oli...</td>\n",
       "      <td>forecast method fertilizer consumption</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SLEIMAN, HASSAN A.&amp;Rafael Corchuelo</td>\n",
       "      <td>survey region extractor web document</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alexandre Solon Nery&amp;Nadia Nedjah&amp;Felipe M. G....</td>\n",
       "      <td>efficient hardware implementation ray trace ba...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Lucenildo Lins de Aquino Júnior&amp;Ruan Gomes&amp;Man...</td>\n",
       "      <td>software base solution distribute film</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Odette Mestrinho Passos&amp;Arilo Claudio Dias Net...</td>\n",
       "      <td>relevant organizational value implementation s...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Délia Gouveia-Reis&amp;Luiz Carlos Guerreiro Lopes...</td>\n",
       "      <td>model maximum rainfall island</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Rafael Ferreira Leite de Mello&amp;Fred Freitas&amp;Pa...</td>\n",
       "      <td>architecture center framework develop blog cra...</td>\n",
       "      <td>2013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              autores  \\\n",
       "3      Egon Walter Wildauer&Fabiana Hoffmann da Silva   \n",
       "6   Davide Vega&Roc Meseguer&Sergio Fabian Ochoa&J...   \n",
       "7                                     Pauline Provini   \n",
       "8   Eduardo Ogasawara&Daniel Cardoso Moraes de Oli...   \n",
       "9                 SLEIMAN, HASSAN A.&Rafael Corchuelo   \n",
       "11  Alexandre Solon Nery&Nadia Nedjah&Felipe M. G....   \n",
       "14  Lucenildo Lins de Aquino Júnior&Ruan Gomes&Man...   \n",
       "17  Odette Mestrinho Passos&Arilo Claudio Dias Net...   \n",
       "18  Délia Gouveia-Reis&Luiz Carlos Guerreiro Lopes...   \n",
       "20  Rafael Ferreira Leite de Mello&Fred Freitas&Pa...   \n",
       "\n",
       "                                               titulo   ano  \n",
       "3                    privacy security issue e society  2013  \n",
       "6   share hardware resource heterogeneous computer...  2013  \n",
       "7                     study system bird origin flight  2013  \n",
       "8              forecast method fertilizer consumption  2013  \n",
       "9                survey region extractor web document  2013  \n",
       "11  efficient hardware implementation ray trace ba...  2013  \n",
       "14             software base solution distribute film  2013  \n",
       "17  relevant organizational value implementation s...  2013  \n",
       "18                      model maximum rainfall island  2013  \n",
       "20  architecture center framework develop blog cra...  2013  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:39.988735Z",
     "start_time": "2019-03-15T19:06:39.950540Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre = df_pre.drop_duplicates(subset=['titulo'],keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:40.036262Z",
     "start_time": "2019-03-15T19:06:40.007288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2133, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:40.083789Z",
     "start_time": "2019-03-15T19:06:40.053997Z"
    }
   },
   "outputs": [],
   "source": [
    "artigos = df_pre['titulo'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:40.134165Z",
     "start_time": "2019-03-15T19:06:40.104756Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('../datasets/artigos/preprocessed/'+year+'/artigosPre.txt','w')\n",
    "f.write(\"\".join(artigos+\"\\n\"))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separar em grupos procurando artigos iguais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T19:06:40.198917Z",
     "start_time": "2019-03-15T19:06:40.154337Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre.to_csv('../datasets/artigos/preprocessed/'+year+'/artigosPre.csv',sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
