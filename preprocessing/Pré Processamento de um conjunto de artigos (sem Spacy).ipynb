{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré Processamento de um conjunto de artigos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolher uma base de artigos e realizar o pré-processamento, sem utilizar a biblioteca Spacy para ajudar na remoção de palavras.\n",
    "\n",
    "O pré processamento se resume em retirar stop words, normalizar o texto, remover números, e remover ruidos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:57:17.107522Z",
     "start_time": "2019-03-15T14:57:16.455393Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'enchant'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e0b12d554699>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'enchant'"
     ]
    }
   ],
   "source": [
    "# Bibliotecas nativas\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import unicodedata\n",
    "import xml.etree.ElementTree as ET\n",
    "import zipfile\n",
    "import pickle\n",
    "# Pré processamento e estruturas de dados\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from fuzzywuzzy import fuzz\n",
    "from langdetect import DetectorFactory, detect\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "import enchant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:52:36.838126Z",
     "start_time": "2019-03-15T14:52:35.282Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en',disable=['parser','ner'])\n",
    "nlp_pt = spacy.load('pt_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:52:36.843768Z",
     "start_time": "2019-03-15T14:52:35.621Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:52:36.849580Z",
     "start_time": "2019-03-15T14:52:36.219Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "def detect_portuguese_text(text):\n",
    "    try:\n",
    "        language = detect(text)\n",
    "    except:\n",
    "        return np.nan\n",
    "    if language == 'en':\n",
    "        return text\n",
    "    else:\n",
    "        return np.nan\n",
    "def remove_portuguese_text(text):\n",
    "    words = set(nltk.corpus.words.words())\n",
    "    \n",
    "    return \" \".join(w for w in nltk.wordpunct_tokenize(text)\n",
    "                    if w.lower() in words or not w.isalpha())\n",
    "def remove_useless_words(text):\n",
    "    doc = nlp(text)\n",
    "    text = \"\"\n",
    "    for token in doc:\n",
    "        if token.pos_ in ['VERB'] and not token.is_stop:\n",
    "            text += token.lemma_ + \" \"\n",
    "    return text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "#     text = remove_portuguese_text(text)\n",
    "#     text = remove_useless_words(text)\n",
    "#     text = detect_portuguese_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:52:36.854923Z",
     "start_time": "2019-03-15T14:52:36.720Z"
    },
    "code_folding": [
     9
    ],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    \n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word.lemma_).encode(\n",
    "            'ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "#def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lemma_.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "def remove_number(words):\n",
    "    text = []\n",
    "    for word in words:\n",
    "        try:\n",
    "            if not word.isdigit():\n",
    "                text.append(word)\n",
    "        except ValueError:\n",
    "            print(word)\n",
    "    return text    \n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    #words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    #words = remove_number(words)\n",
    "#     words = replace_numbers(words)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-15T14:52:37.882519Z",
     "start_time": "2019-03-15T14:52:37.856207Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enchant' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b02cca8a7689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menchant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_US'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_portuguese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0menglish_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enchant' is not defined"
     ]
    }
   ],
   "source": [
    "def remove_small_words(wordst):\n",
    "    new_words = []\n",
    "    for word in wordst:\n",
    "        if len(word) > 1:\n",
    "            #print(word)\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "def remove_useless_words(text):\n",
    "    doc = nlp(text)\n",
    "    text = \"\"\n",
    "    lemma = ['ADJ','NOUN']\n",
    "    for token in doc:\n",
    "            if token.pos_ not in ['ADJ']:\n",
    "                text += token.lemma_ + \" \"\n",
    "    return text\n",
    "def remove_stopwords_pt(wordst):\n",
    "    stop = stopwords.words('portuguese')\n",
    "    stop = remove_non_ascii(stop)\n",
    "    new_words = [w for w in wordst if w not in stop]\n",
    "\n",
    "    return new_words\n",
    "def remove_stopwords_en(wordst):\n",
    "    stop = stopwords.words('english')\n",
    "    stop = remove_non_ascii(stop)\n",
    "    new_words = [w for w in wordst if w not in stop]\n",
    "\n",
    "    return new_words\n",
    "\n",
    "d = enchant.Dict('en_US')\n",
    "def remove_portuguese(words):\n",
    "    english_words = []\n",
    "    totalLength = len(words)\n",
    "    cont = 0\n",
    "    for word in words:\n",
    "        word = word.lemma_\n",
    "        if d.check(word):\n",
    "            english_words.append(word)\n",
    "        else:\n",
    "            cont+= 1\n",
    "   # print(len(english_words),totalLength)\n",
    "    if totalLength > 0:        \n",
    "        rate = cont/totalLength\n",
    "    else: \n",
    "        rate = 0\n",
    "    #print(rate)\n",
    "    if (rate) > 0.2:\n",
    "        new_words = []\n",
    "        return new_words\n",
    "    else:\n",
    "        return english_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicialização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:04.149449Z",
     "start_time": "2019-02-02T18:00:04.021242Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leandro/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>autores</th>\n",
       "      <th>titulo</th>\n",
       "      <th>ano</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GARCIA, LUAN FONSECA&amp;GRACIOLLI, VINICIUS&amp;DE RO...</td>\n",
       "      <td>A Conceptual Framework for Rock Data Integrati...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Araújo, Ricardo de A.&amp;Adriano Lorena Inacio de...</td>\n",
       "      <td>A morphological neural network for binary clas...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALVES PEREIRA, LUIS F.&amp;JANSSENS, ELINE&amp;George ...</td>\n",
       "      <td>Inline discrete tomography system: Application...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Túlio Ângelo Machado Toffolo&amp;ESPRIT, ELINE&amp;Ton...</td>\n",
       "      <td>A two-dimensional heuristic decomposition appr...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ROSEMBACK, ROBERTA GUERRA&amp;RANGEL RIGOTTI, JOSÉ...</td>\n",
       "      <td>Demografia, planejamento territorial e a quest...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MAROTTA, MARCELO A.&amp;KIST, MAICON&amp;WICKBOLDT, JU...</td>\n",
       "      <td>Design considerations for software-defined wir...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SILVA, GUSTAVO R.L.&amp;MEDEIROS, RAFAEL R.&amp;JAIMES...</td>\n",
       "      <td>CUDA-based parallelization of Power Iteration ...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Adrialdo Azanha&amp;João Batista de Camargo Junior...</td>\n",
       "      <td>ERP: uma investigacao sobre a decisao entre co...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Rogério Ricalde Torres&amp;Taise Cristine Buske&amp;To...</td>\n",
       "      <td>Alem do equipamento</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DOUGLAS, DAVID&amp;SANTANNA, JOSÉ JAIR&amp;Ricardo de ...</td>\n",
       "      <td>Booters: can anything justify distributed deni...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             autores  \\\n",
       "0  GARCIA, LUAN FONSECA&GRACIOLLI, VINICIUS&DE RO...   \n",
       "1  Araújo, Ricardo de A.&Adriano Lorena Inacio de...   \n",
       "2  ALVES PEREIRA, LUIS F.&JANSSENS, ELINE&George ...   \n",
       "3  Túlio Ângelo Machado Toffolo&ESPRIT, ELINE&Ton...   \n",
       "4  ROSEMBACK, ROBERTA GUERRA&RANGEL RIGOTTI, JOSÉ...   \n",
       "5  MAROTTA, MARCELO A.&KIST, MAICON&WICKBOLDT, JU...   \n",
       "6  SILVA, GUSTAVO R.L.&MEDEIROS, RAFAEL R.&JAIMES...   \n",
       "7  Adrialdo Azanha&João Batista de Camargo Junior...   \n",
       "8  Rogério Ricalde Torres&Taise Cristine Buske&To...   \n",
       "9  DOUGLAS, DAVID&SANTANNA, JOSÉ JAIR&Ricardo de ...   \n",
       "\n",
       "                                              titulo   ano  \n",
       "0  A Conceptual Framework for Rock Data Integrati...  2017  \n",
       "1  A morphological neural network for binary clas...  2017  \n",
       "2  Inline discrete tomography system: Application...  2017  \n",
       "3  A two-dimensional heuristic decomposition appr...  2017  \n",
       "4  Demografia, planejamento territorial e a quest...  2017  \n",
       "5  Design considerations for software-defined wir...  2017  \n",
       "6  CUDA-based parallelization of Power Iteration ...  2017  \n",
       "7  ERP: uma investigacao sobre a decisao entre co...  2017  \n",
       "8                                Alem do equipamento  2017  \n",
       "9  Booters: can anything justify distributed deni...  2017  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_artigos = pd.read_csv('../datasets/artigos/nopreprocessed/artigos_2017.csv',sep=\"\\*\\|\\*\")\n",
    "df_artigos.dropna(inplace=True)\n",
    "df_artigos = df_artigos.reset_index(drop=True)\n",
    "\n",
    "df_artigos = df_artigos.sample(\n",
    "    frac=1, random_state=29).reset_index(drop=True)  # Embaralha (shuffle) as linhas\n",
    "\n",
    "df_artigos.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:04.174147Z",
     "start_time": "2019-02-02T18:00:04.150872Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_pre = df_artigos.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-01T16:43:55.324755Z",
     "start_time": "2019-02-01T16:43:55.312922Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:04.222030Z",
     "start_time": "2019-02-02T18:00:04.176350Z"
    }
   },
   "outputs": [],
   "source": [
    "#df_pre['titulo'] = df_pre['titulo'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:19.816698Z",
     "start_time": "2019-02-02T18:00:04.224868Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(denoise_text)\n",
    "\n",
    "df_pre['titulo'] = df_pre['titulo'].apply(remove_useless_words)\n",
    "\n",
    "df_pre['titulo'] = df_pre['titulo'].apply(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:19.822376Z",
     "start_time": "2019-02-02T18:00:19.818394Z"
    }
   },
   "outputs": [],
   "source": [
    "def new_normalize(tokens):\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token.is_ascii and not token.is_digit and not token.is_punct and not token.is_stop and token.is_alpha:\n",
    "            new_tokens.append(token)\n",
    "    return new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:19.979360Z",
     "start_time": "2019-02-02T18:00:19.823846Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(new_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:20.799939Z",
     "start_time": "2019-02-02T18:00:19.981128Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(remove_portuguese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:20.807806Z",
     "start_time": "2019-02-02T18:00:20.801906Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre = df_pre[df_pre['titulo'].apply(len) >= 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:20.912679Z",
     "start_time": "2019-02-02T18:00:20.809595Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre['titulo'] = df_pre['titulo'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:20.975115Z",
     "start_time": "2019-02-02T18:00:20.914578Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>autores</th>\n",
       "      <th>titulo</th>\n",
       "      <th>ano</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GARCIA, LUAN FONSECA&amp;GRACIOLLI, VINICIUS&amp;DE RO...</td>\n",
       "      <td>conceptual framework rock datum integration re...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Araújo, Ricardo de A.&amp;Adriano Lorena Inacio de...</td>\n",
       "      <td>network classification problem</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ALVES PEREIRA, LUIS F.&amp;JANSSENS, ELINE&amp;George ...</td>\n",
       "      <td>tomography system application product inspection</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Túlio Ângelo Machado Toffolo&amp;ESPRIT, ELINE&amp;Ton...</td>\n",
       "      <td>decomposition approach container load problem</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MAROTTA, MARCELO A.&amp;KIST, MAICON&amp;WICKBOLDT, JU...</td>\n",
       "      <td>design consideration software define network c...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>FERNANDES, CHRYSTINNE OLIVEIRA&amp;Carlos José Per...</td>\n",
       "      <td>software framework remote patient monitor use ...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Julio Cesar Santos dos Anjos&amp;Tatiana Galibus&amp;C...</td>\n",
       "      <td>sec approach secure big datum process cloud</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Leyvison Rafael V. da Conceição&amp;Livia M. Carne...</td>\n",
       "      <td>synthesis macaw palm oil use catalyst comprise...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Gildárcio Sousa Gonçalves&amp;Luiz Alberto Vieira ...</td>\n",
       "      <td>agile interdisciplinary approach safety critic...</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>THOMAZINHO, HELLEN CHRISTINE SERODIO&amp;Alexandre...</td>\n",
       "      <td>case study strategy maintain software numb user</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              autores  \\\n",
       "0   GARCIA, LUAN FONSECA&GRACIOLLI, VINICIUS&DE RO...   \n",
       "1   Araújo, Ricardo de A.&Adriano Lorena Inacio de...   \n",
       "2   ALVES PEREIRA, LUIS F.&JANSSENS, ELINE&George ...   \n",
       "3   Túlio Ângelo Machado Toffolo&ESPRIT, ELINE&Ton...   \n",
       "5   MAROTTA, MARCELO A.&KIST, MAICON&WICKBOLDT, JU...   \n",
       "13  FERNANDES, CHRYSTINNE OLIVEIRA&Carlos José Per...   \n",
       "15  Julio Cesar Santos dos Anjos&Tatiana Galibus&C...   \n",
       "17  Leyvison Rafael V. da Conceição&Livia M. Carne...   \n",
       "19  Gildárcio Sousa Gonçalves&Luiz Alberto Vieira ...   \n",
       "20  THOMAZINHO, HELLEN CHRISTINE SERODIO&Alexandre...   \n",
       "\n",
       "                                               titulo   ano  \n",
       "0   conceptual framework rock datum integration re...  2017  \n",
       "1                      network classification problem  2017  \n",
       "2    tomography system application product inspection  2017  \n",
       "3       decomposition approach container load problem  2017  \n",
       "5   design consideration software define network c...  2017  \n",
       "13  software framework remote patient monitor use ...  2017  \n",
       "15        sec approach secure big datum process cloud  2017  \n",
       "17  synthesis macaw palm oil use catalyst comprise...  2017  \n",
       "19  agile interdisciplinary approach safety critic...  2017  \n",
       "20    case study strategy maintain software numb user  2017  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:21.026545Z",
     "start_time": "2019-02-02T18:00:20.977324Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_pre = df_pre.drop_duplicates(subset=['titulo'],keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:21.076426Z",
     "start_time": "2019-02-02T18:00:21.031757Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2226, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:21.125978Z",
     "start_time": "2019-02-02T18:00:21.083021Z"
    }
   },
   "outputs": [],
   "source": [
    "artigos = df_pre['titulo'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:21.177685Z",
     "start_time": "2019-02-02T18:00:21.129197Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('../datasets/artigos/preprocessed/2017/artigosPre.txt','w')\n",
    "f.write(\"\".join(artigos+\"\\n\"))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separar em grupos procurando artigos iguais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-02T18:00:21.243186Z",
     "start_time": "2019-02-02T18:00:21.179426Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pre.to_csv('../datasets/artigos/preprocessed/2017/artigosPre.csv',sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
